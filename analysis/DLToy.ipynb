{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "with open('datacompiled.p', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PolFull = []\n",
    "AziFull = []\n",
    "FitFull = []\n",
    "EpsFull = []\n",
    "for nanoparticle in data:\n",
    "    FitFull.append(float(nanoparticle[0]))\n",
    "    for ligand in nanoparticle[1]:\n",
    "        PolFull.append(ligand[0])\n",
    "        AziFull.append(ligand[1])\n",
    "        EpsFull.append(ligand[2])\n",
    "PolMax = max(PolFull)\n",
    "PolMin = min(PolFull)\n",
    "AziMax = max(AziFull)\n",
    "AziMin = min(AziFull)\n",
    "FitMax = max(FitFull)\n",
    "FitMin = min(FitFull)\n",
    "EpsMax = max(EpsFull)\n",
    "EpsMin = min(EpsFull)\n",
    "discreteBlocks = 8\n",
    "discreteBlocksFit = 10\n",
    "PolStep = (PolMax-PolMin)/discreteBlocks\n",
    "AziStep = (AziMax-AziMin)/discreteBlocks\n",
    "FitStep = (FitMax-FitMin)/discreteBlocksFit\n",
    "#x = Pol, y=Azi\n",
    "def getIndex(Pol, Azi, PolMin, AziMin, PolStep, AziStep, discreteBlocks):    \n",
    "    if Pol > 6.258: ###!!!! HCAK: to stop border values getting pushed over for now :(\n",
    "        xGrid = discreteBlocks - 1\n",
    "    else:\n",
    "        xGrid = math.floor((Pol - PolMin)/PolStep) #0 indexed grid\n",
    "    if Azi > 6.258: ###!!!! HCAK: to stop border values getting pushed over for now :(\n",
    "        yGrid = discreteBlocks - 1\n",
    "    else:\n",
    "        yGrid = math.floor((Azi - AziMin)/AziStep)    \n",
    "    #print('x:{}, y:{}'.format(xGrid, yGrid))\n",
    "    index = (yGrid*discreteBlocks + xGrid)\n",
    "    return index\n",
    "def roundDownFit(Fit, FitMin, FitStep, discreteBlocks):\n",
    "    return (math.floor((Fit - FitMin)/FitStep))*FitStep  \n",
    "def roundDownEps(Eps):\n",
    "    return float(int(Eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFull = []\n",
    "outputFull = []\n",
    "for nanoparticle in data:\n",
    "    inputFull.append(nanoparticle[1])\n",
    "    outputFull.append(float(nanoparticle[0]))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDiscrete = []\n",
    "for output_i in outputFull:\n",
    "    #outputDiscrete.append(roundDownFit(output_i, FitMin, FitStep, discreteBlocksFit))\n",
    "    outputDiscrete.append(output_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(outputFull, '.', label='full')\n",
    "plt.plot(outputDiscrete, '.',label='Discretised')\n",
    "plt.legend()\n",
    "plt.title('Fit Discretised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDiscrete = []\n",
    "for ligands in inputFull:\n",
    "    inputDiscreteBlock = [0]*discreteBlocks**2\n",
    "    for ligand in ligands:        \n",
    "        updateIndex = getIndex(ligand[0],ligand[1], PolMin, AziMin, PolStep, AziStep, discreteBlocks)\n",
    "        discreteEps = roundDownEps(ligand[2])   \n",
    "        #print('adding eps {} at index {} for pol {} and azi {}'.format(discreteEps, updateIndex, ligand[0], ligand[1]))        \n",
    "        inputDiscreteBlock[updateIndex] += discreteEps\n",
    "    inputDiscrete.append(inputDiscreteBlock)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(inputDiscrete[0], '.')\n",
    "#print((inputFull[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customSample(data):\n",
    "    random.shuffle(data)\n",
    "    return data.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(inputDiscrete))\n",
    "print(len(outputDiscrete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_size = 100\n",
    "validation_input = []\n",
    "validation_output = []\n",
    "training_input = []\n",
    "training_output = []\n",
    "for i in range(validation_size):\n",
    "    validation_input.append(customSample(inputDiscrete))        \n",
    "    validation_output.append(customSample(outputDiscrete))\n",
    "training_input = inputDiscrete\n",
    "training_output = outputDiscrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_input=np.array(validation_input)\n",
    "print('validation input shape:{}'.format(np.shape(validation_input)))\n",
    "validation_output=(np.array(validation_output)).reshape(np.shape(validation_output)[0],1)\n",
    "print('validation output shape:{}'.format(np.shape(validation_output)))\n",
    "training_input=np.array(training_input)\n",
    "print('training input shape:{}'.format(np.shape(training_input)))\n",
    "training_output=(np.array(training_output)).reshape(np.shape(training_output)[0],1)\n",
    "print('training output shape:{}'.format(np.shape(training_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(sample_length, list1, list2):\n",
    "    len_list1 = np.shape(list1)[0]\n",
    "    len_list2 = np.shape(list2)[0]\n",
    "    if len_list1 != len_list2:\n",
    "        return False\n",
    "    else:\n",
    "        sample_idx = random.sample(range(len_list1), sample_length)        \n",
    "        s_list1 = [list1[i] for i in sample_idx]\n",
    "        s_list2 = [list2[i] for i in sample_idx]\n",
    "        return s_list1, s_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "input_count = 64\n",
    "inputs = tf.placeholder(tf.float32, shape=[None, input_count])\n",
    "\n",
    "output_count = 1\n",
    "desired_outputs = tf.placeholder(tf.float32, shape=[None, output_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hidden layer 1\n",
    "weights_mean_HL1 = 0\n",
    "weights_mean_OUT = 0\n",
    "\n",
    "weights_mean_HL2 = 0\n",
    "weights_mean_OUT = 0\n",
    "\n",
    "HL1_count = 25\n",
    "HL2_count = 9\n",
    "\n",
    "weights_HL1 = tf.Variable(tf.truncated_normal([input_count, HL1_count], weights_mean_HL1))\n",
    "biases_HL1 = tf.Variable(tf.zeros([HL1_count]))\n",
    "HL1_out = tf.nn.relu(tf.matmul(inputs, weights_HL1) + biases_HL1)\n",
    "\n",
    "weights_HL2 = tf.Variable(tf.truncated_normal([HL1_count, HL2_count], weights_mean_HL2))\n",
    "biases_HL2 = tf.Variable(tf.zeros([HL2_count]))\n",
    "HL2_out = tf.nn.relu(tf.matmul(HL1_out, weights_HL2) + biases_HL2)\n",
    "\n",
    "weights_out = tf.Variable(tf.truncated_normal([HL2_count, output_count], weights_mean_OUT))\n",
    "biases_out = tf.Variable(tf.zeros([output_count]))\n",
    "net_out = tf.nn.relu(tf.matmul(HL2_out, weights_out) + biases_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_function = tf.reduce_mean(tf.square(tf.subtract(net_out, desired_outputs)))\n",
    "#train_step = tf.train.GradientDescentOptimizer(0.0001).minimize(error_function)\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(error_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restorer = tf.train.Saver()\n",
    "# restorer.restore(sess, './tfmodel-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_loss = []\n",
    "train_length = 100000\n",
    "sample_length = 2000\n",
    "\n",
    "start_train_sess = time.clock()\n",
    "for i in range(train_length):\n",
    "    sample_inputs, sample_outputs = random_sample(sample_length, training_input, training_output)\n",
    "    _, loss = sess.run([train_step, error_function], \n",
    "                       feed_dict={inputs: np.array(sample_inputs), desired_outputs: np.array(sample_outputs)})\n",
    "\n",
    "    if i%(train_length/10) == 0:\n",
    "        print ('training step: {0}, loss:{1}'.format(i, loss))        \n",
    "\n",
    "    track_loss.append(loss)\n",
    "\n",
    "print('training session:{0}s for {1} steps'.format((time.clock() - start_train_sess), train_length))\n",
    "\n",
    "plt.plot(track_loss, 'o')    \n",
    "print('final loss:{0}'.format(track_loss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_predict = sess.run(net_out, feed_dict={inputs: np.array(validation_input)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(net_predict, '.', label='predicted')\n",
    "plt.plot(validation_output, '.', label='ground truth')\n",
    "plt.legend()\n",
    "plt.xlabel('arb nanoparticle index')\n",
    "plt.ylabel('fitness')\n",
    "plt.show()\n",
    "\n",
    "tolerance = FitStep\n",
    "withinTol = 0\n",
    "difference = []\n",
    "for predict_i, truth_i in zip(net_predict, validation_output):\n",
    "    difference.append(np.abs(predict_i - truth_i))\n",
    "    if np.abs(predict_i - truth_i) <= tolerance:\n",
    "        withinTol += 1\n",
    "print('{}/{} within tolerance of {}'.format(withinTol, len(net_predict), tolerance))\n",
    "plt.plot(difference, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(validation_output, '.', label='validation set', alpha=1)\n",
    "# plt.plot(training_output, '.',label='training set', alpha=0.5)\n",
    "# plt.legend()\n",
    "# plt.xlabel('arb nanoparticle index')\n",
    "# plt.ylabel('fitness')\n",
    "# plt.xlim(0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_HL1_save = sess.run(weights_HL1, feed_dict={inputs: np.array(training_inputs)})\n",
    "# biases_HL1_save = sess.run(biases_HL1, feed_dict={inputs: np.array(training_inputs)})\n",
    "# weights_out_save = sess.run(weights_out, feed_dict={inputs: np.array(training_inputs)})\n",
    "# biases_out_save = sess.run(biases_out, feed_dict={inputs: np.array(training_inputs)})\n",
    "\n",
    "# pickle.dump (weights_HL1_save, open(\"./tfmodel-0-weights_HL1.pickle\", \"wb\"))\n",
    "# pickle.dump (biases_HL1_save, open(\"./tfmodel-0-biases_HL1.pickle\", \"wb\"))\n",
    "# pickle.dump (weights_out_save, open(\"./tfmodel-0-weights_out.pickle\", \"wb\"))\n",
    "# pickle.dump (biases_out_save, open(\"./tfmodel-0-biases_out.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(tf.global_variables())\n",
    "saver.save(sess, './tfmodel-hl3-adam', global_step = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
